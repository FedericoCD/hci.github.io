<!DOCTYPE HTML>

<html>
	<head>
		<title>Human Computer Interaction - AI Bias</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="homepage is-preload">
		<div id="page-wrapper">

			<!-- Header -->
				<section id="header">
					<div class = "header-container">
						<div class = "top-bar">
						<!-- Logo -->
							<h1><a href="#header">Human Computer Interaction</a></h1>

						<!-- Nav -->
							<nav id="nav">
								<ul>
									<li><a href="#case1">Gender Bias</a></li>
									<li><a href="#case2">Racial Bias</a></li>
								</ul>
							</nav>
						</div>
					</div>

					<!-- Banner -->
						<section id="banner">
							<header>
								<h2>Measuring AI Bias</h2>
								<p>PROJECT 2 - 2021</p>
							</header>
						</section>

						<section id="main">

							<div class="container" id= "intro">
		
								<!-- Content -->
									<article class="box post">
										<!-- <header>
											<h2>Introduction</h2>
										</header> -->
										<p>
											COCO Captions is a large open-source dataset for object detection, segmentation, and automatic caption generation. We used COCO 2014 which contains over 30k human-annotated images for training and 5k for validation and testing. To analyze gender bias in COCO Captions, we used a data exploration tool, <a href="https://knowyourdata.withgoogle.com" target="_blank">Know Your Data (KYD)</a> , which is a publicly available tool developed by People + AI Research (a division of Google I/O) to help researchers understand datasets with the goal of improving data quality and help mitigate fairness and bias issues.
										</p>
										<p>
											The scores obtained in KYD are based on the PMI metric to <a href="https://arxiv.org/pdf/2103.03417.pdf" target="_blank">measure associations learned by a model</a>  and it helps us understand the extent to which two words in a dataset co-occur more or less than would be expected by chance. For example, exploring the co-occurrence of the word “sport” and “female” or “male”, a score of [1, 1] is telling us that “sport” is fairly (as expected by chance) represented in the annotated captions, however, a score of [0.3, 2] is telling us that the word is underrepresented for “female” and overrepresented for “male”. 
										</p>

									</article>
		
							</div>

							<div class="container" id= "case1">
		
								<!-- Content -->
									<article class="box post">
										<!-- <a href="#" class="image featured"><img src="images/pic01.jpg" alt="" /></a> -->
										<header>
											<h2>Exploring Gender Bias</h2>
											<p>COCO Captions dataset</p>
										</header>
										
										<p>
											COCO Captions is a large open-source dataset for object detection, segmentation, and automatic caption generation. We used COCO 2014 which contains over 30k human-annotated images for training and 5k for validation and testing. To analyze gender bias in COCO Captions, we used a data exploration tool, <a href="https://knowyourdata.withgoogle.com" target="_blank">Know Your Data (KYD)</a> , which is a publicly available tool developed by People + AI Research (a division of Google I/O) to help researchers understand datasets with the goal of improving data quality and help mitigate fairness and bias issues.
										</p>
										<p>
											The scores obtained in KYD are based on the PMI metric to <a href="https://arxiv.org/pdf/2103.03417.pdf" target="_blank">measure associations learned by a model</a>  and it helps us understand the extent to which two words in a dataset co-occur more or less than would be expected by chance. For example, exploring the co-occurrence of the word “sport” and “female” or “male”, a score of [1, 1] is telling us that “sport” is fairly (as expected by chance) represented in the annotated captions, however, a score of [0.3, 2] is telling us that the word is underrepresented for “female” and overrepresented for “male”. 
										</p>
										<div id="chartdiv1"></div>
										<!-- <div id="chartdiv0"></div> -->

										<strong>Figure 1. Co-occurrence score for gender captioned images</strong>

										<section>
											<header>
												<h3>Interpreting the chart</h3>
											</header>
											<p>
												For the analysis, we considered two main groups of captions words. First, we examined images captioned with words describing different activities and studied their relationship with gendered caption words such as “male” and “female”. The second group is caption words associated with physical appearance. It is worth noting that these are captions generated by human annotators who are making subjective assessments about gender and choosing words that, based on their judgment, better describe what is contained in the image. The graph allows us to directly compare the scores obtained for words within the two groups, in addition, to clearly highlighting the difference between "female" and "male". 
	
											</p>
											<p>
												For activities stereotypically associated with the female gender, such as "decorating", "sewing" and "shopping”, we found images captioned with “female” or “woman” at a much higher rate than images captioned with “male” or “man”. In contrast, captions describing activities considered as physically intensive, such as “soccer”, “baseball” and “athlete”, co-occur with images captioned with “male” or “man” at a much higher rate. Other interesting subgroups that we found are words related to cooking, such as "dinner" and "dishes", with a strong correlation with "female" and words related to professional careers, such as "engineer" and "pilot", with an overrepresentation of images captioned with "male". While individual image captions may not use stereotypical or derogatory language if certain gender groups are over (or under) represented within a particular activity across the whole dataset, models developed from the dataset risk learning stereotypical associations.
											</p>
											<p>
												Regarding adjectives that describe physical appearance, we found annotators having a tendency to choose words related to attractiveness in co-occurrence with "female" or "woman”. As can be seen in the graph, words such as "sexy", "adorable" and "attractive" are overrepresented in the dataset for the female gender and underrepresented for the male gender. A model trained with this data would have a strong inclination to caption “attractiveness” in images containing women, without even taking into account bias associated with assuming a person's gender based solely on appearance.
											</p>
										</section>

									</article>
		
							</div>

							<div class="container" id="case2">

								<!-- Content -->
								<article class="box post">
									<!-- <a href="#" class="image featured"><img src="images/pic01.jpg" alt="" /></a> -->
									<header>
										<h2>Analyzing Racial Bias</h2>
										<p>COMPAS Dataset</p>
									</header>
									<p>
										COMPAS - Correctional Offender Management Profiling for Alternative Sanctions is an algorithm used by judges and parole officers for scoring a defendant’s likelihood of repeating a crime – reoffending or recidivism. The database used for this project includes criminal history, jail and prison time, demographics and COMPAS risk scores for defendants from Broward County from 2013 and 2014.  The algorithm has been proven to be biased in favor of white defendants and highly against the black defendants. This study was conducted for 2 years, and pattern of mistakes were notable.  For this study, more than 10,000 criminal defendants were observed and compared the predicted recidivism rate to the actual recidivism that occurred. When defendants are booked in jail, the COMPAS questionnaire is filled up and depending on the answers which are fed to the system, risk of recidivism and risk of violent recidivism were calculated. The algorithm correctly predicted risk of recidivism only 61% and for risk of violent recidivism, only 20% were correctly predicted.  
									</p>
									<div id="chartdiv2"></div>
									<strong class= "caption">Figure 2. Distribution of risk score for criminals with different races</strong>
									<p>The graph above shows the risk score distribution for different races. The labels indicate risk score: whether the risk is low, medium, or high. The graph indicates that white and black defendants have the highest count for medium to high risk. 14.4% of the black defendants were labeled with a high risk score.
									</p>
									<div id="chartdiv5"></div>
									<strong>Figure 3. Distribution of decile score for White defendants versus Black defendants</strong>
									<p>
									This graph indicates the decile score for black versus white defendants. If we compare the decile score distribution for white defendants, we see that it’s highly skewed towards lower risk. For black defendants, the decile score distribution has been equally distributed.
									</p>
									<div id="chartdiv4"></div>
									<strong>Figure 4. Co-occurrence score for gender captioned images</strong>
									<p>
									The pie charts above are used to indicate the number of defendants predicted for risk of recidivism and risk of violent recidivism. I’ve considered the defendants which were predicted to be risky and had a prior crime history of 0 or 1 criminal record. As the graph indicates, the number of black defendants have the highest percentage of being identified as risky in comparison to other races.
									</p>
									<p>
									All these graphs indicate the bias towards black people. Blacks are highly likely to be misclassified as higher risk of violent recidivism, and white recidivists are misclassified as low risk more often than black defendants.  
									</p>
									<p>
										The software used across the country to predict future criminals is biased towards blacks.
										The ML model classification is shown as below:

									</p>
									<table>
										<tr>
										  <th></th>
										  <th>White</th> 
										  <th>African American</th>
										</tr>
										<tr>
										  <td> <strong>Labeled Higher Risk, But Didn't Re-Offend</strong> </td>
										  <td>23.5%</td> 
										  <td>44.9%</td>
										</tr>
										<tr>
										  <td> <strong> Labeled Lower Risk, Yet Did Re-Offend</strong></td>
										  <td>47.7%</td> 
										  <td>28.0%</td>
										</tr>
									</table>
							
								</article>
		
							</div>
						</section>

				</section>


			<!-- Footer -->
				<section id="footer">
					<div class="container">
						<div class="row">
							<div class="col-8 col-12-medium">
								<section>
									<header>
										<h2>References</h2>
									</header>
									<ul class="dates">
										<li>
											<span class="date"><strong>1</strong></span>
											<h3><a href="#">Common Objects in Context - COCO</a></h3>
											<p>Chen, X. (2015, April 1). Microsoft COCO Captions: Data Collection and Evaluation Server.  <a href="https://arxiv.org/abs/1504.00325" target="_blank">https://arxiv.org/abs/1504.00325</a></p>
										</li>
										<li>
											<span class="date"><strong>2</strong></span>
											<h3><a href="#">Measuring Model Biases in the Absence of Ground Truth</a></h3>
											<p>Aka, O. (2021, June). <a href="https://arxiv.org/pdf/2103.03417.pdf" target="_blank"></a> https://arxiv.org/pdf/2103.03417.pdf</p>
										</li>
										<li>
											<span class="date"><strong>3</strong></span>
											<h3><a href="#">Magna tempus lorem feugiat</a></h3>
											<p>Dolore consequat sed phasellus lorem sed etiam nullam dolor etiam sed amet sit consequat.</p>
										</li>
										<li>
											<span class="date"><strong>4</strong></span>
											<h3><a href="#">Dolore tempus ipsum feugiat nulla</a></h3>
											<p>Feugiat lorem dolor sed nullam tempus lorem ipsum dolor sit amet nullam consequat.</p>
										</li>
										<li>
											<span class="date"><strong>5</strong></span>
											<h3><a href="#">Blandit tempus aliquam?</a></h3>
											<p>Feugiat sed tempus blandit tempus adipiscing nisl lorem ipsum dolor sit amet dolore.</p>
										</li>
									</ul>
								</section>
							</div>
						</div>
					</div>
				</section>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
			<script src="core.js"></script>
			<script src="charts.js"></script>
			<script src="themes/animated.js"></script>
			<script src="column-chart.js"></script>
			<script src="area-radar.js"></script>
			<script src="stackedcharts.js"></script>
			<script src="clusteredchart.js"></script>
			<script src="pieofpie-twoyear.js"></script>
			<script src="pieofpie-violent.js"></script>
	</body>
</html>